# Overfitting and Generalization
## The Problem of Overfitting
- **Underfitting**:
- - The model has a high bias
- - housing prices will vary(thay doi) linearly
with their size
- - Not fitting
the training data well
- ![alt text](image.png)
- ![alt text](image-1.png)
- - Just right
- - It fits the data pretty well
- **Overfitting**
- - The model has a high variance
- - Very poor performance on 
new data
- -  It does not generalize well
- ![alt text](image-2.png)
## The problem of overfitting - classification
- ![alt text](image-3.png)
## The problem of overfitting
- ![alt text](image-4.png)
-  What makes it more likely
to overfit?
- - Not enough training
examples
- - Too many features
- - Using a non-convenient
type of models
## Generalization(khÃ¡i quÃ¡t) Error
- **The training error** is the error of the model $â„_ğœƒ$ on the training examples.
- **The generalization error** is the error of the model $â„_ğœƒ$ on new (unseen) data
- - **The generalization error** is **typically higher** than the **training error**
- How do we estimate the generalization error of some model?
- - Using k-fold Cross Validation (k-CV)
- - Using Leave-One-Out estimate (LOO)
- Example of k=5-fold cross validation:
- - Split the training dataset into 5 parts (folds).
- - Each time, train a model on the training
parts (green) and apply it on the
remaining part (blue) to estimate the error
- - So, you will finally get 5 estimates of the error. The generalization error is the average of these 5 errors. In the case of classification, you can also compute the  generalization accuracy(Ä‘á»™ chÃ­nh xÃ¡c) the same way (accuracy% = 100 â€“ error%) 
- ![alt text](image-5.png)
- Leave-one-out cross validation:
- -It is similar to the k-fold cross validation, but the training dataset is split into ğ‘› parts, where ğ‘› is the number of training samples. i.e., we leave one sample each time.
- It is useful when you have a small training dataset (i.e. when ğ‘›ğ‘› is small).
## 1. Addressing overfitting
- when we have more than 2 or 3 features.
1. Model selection / hyperparameters tuning 
2. Reducing the number of features
3. Using an ensemble methods
### Hyper-parameters Tuning
- Most ML algorithm has hyperparameters that lead to(dan toi) a more or less complex model.
- To find the values of hyperparameters that lead to a good model (not too simple, not too complex), we need to:
- - Pick some values for our hyperparameters.
- - Compute the generalization error using a 10-fold-cross-validation
- - Repeat steps 1 and 2
- - Keep the hyperparameters that gave you to the smallest generalization error; and train a model on the whole training set using those hyperparameters.
- - You can now test your trained model on the test data (see figure) to see how it will perform 
- ![alt text](image-6.png)
### Comparing two models
- Suppose that you want to compare two regression algorithms A and B.
- when you performed a 10-fold-cross-validation on A and B, you got the following MSE error estimates:
- - For A: [0.6, 0.1, 1.1, 1.3, 0.3, 3.2, 3.2, 0.9, 1.3, 1.1] . The average is $ğœ‡_ğ´$ = 1.31. The stdev is $ğœ_ğ´$ = 1.01
- - For B: [3.3, 0.6, 5, 12.4, 1.2, 5.8, 8.6, 0.4, 0.5, 4.35]. The average is $ğœ‡_B$ = 4.21. The stdev is $ğœ_ğ´$ = 3.75
- Model A is better than model B  ($ğœ‡_ğ´$ < $ğœ‡_ğµ$).
- Is A significantly (dang ke)better than B?: To answer this question we need to do some statistical tests. One such test (Wilcoxon test) uses 
the standard deviations $ğœ_ğ´$, $ğœ_ğµ$ and compares the means $ğœ‡_ğ´$, $ğœ‡_ğµ$ to check if:
- ![alt text](image-7.png)
-> If true, then the results achieved by A and B are significantly different
## 2. Features Selection
### Motivation behind features selection
- Datasets with thousands or millions of features are quite usual( kha binh thuong) these days.
- Choose features are needed 
- ![alt text](image-8.png)
### Simple (naÃ¯ve) feature selection
- Removing features with low variance:
- Suppose that we have a feature that have the same value in 
all samples (i.e. a column of ğ‘‹ which has a constant value)
- It doesnâ€™t help differencing between samples.
-> Removed
### Recursive(de quy) feature elimination(loai bo)
- Suppose that we have a model that assigns(gans) weights to features
- $( h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \dots + \theta_d x_d )$
- We train a model using the initial set of features
- We obtain(hieu duoc) the importance of each feature
- We remove the least(its) important features from current set of features.
- We repeat this procedure(quy tirnh) recursively(de quy) until the desired(dat duoc) number of features to select is reached
- ![alt text](image-9.png)
- Ban Ä‘áº§u, khi sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng Ä‘Æ°á»£c chá»n tÄƒng lÃªn, Ä‘iá»ƒm sá»‘ cá»§a mÃ´ hÃ¬nh cÅ©ng tÄƒng lÃªn nhanh chÃ³ng.
- Tuy nhiÃªn, sau khi chá»n khoáº£ng 3 Ä‘áº·c trÆ°ng, Ä‘iá»ƒm sá»‘ Ä‘áº¡t giÃ¡ trá»‹ cao nháº¥t (~0.8).
- Sau má»‘c nÃ y, khi thÃªm nhiá»u Ä‘áº·c trÆ°ng hÆ¡n (quÃ¡ 3 Ä‘áº·c trÆ°ng), Ä‘iá»ƒm sá»‘ xÃ¡c thá»±c chÃ©o báº¯t Ä‘áº§u giáº£m nháº¹ hoáº·c khÃ´ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ.
- Trong vÃ­ dá»¥ nÃ y, sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng tá»‘i Æ°u lÃ  3 (Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u báº±ng mÃ u Ä‘á»), vÃ¬ Ä‘Ã¢y lÃ  má»‘c mÃ  mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t nháº¥t trÆ°á»›c khi báº¯t Ä‘áº§u giáº£m sÃºt hoáº·c á»•n Ä‘á»‹nh.
- Viá»‡c chá»n nhiá»u hÆ¡n 3 Ä‘áº·c trÆ°ng khÃ´ng giÃºp cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c, cÃ³ thá»ƒ dáº«n Ä‘áº¿n hiá»‡n tÆ°á»£ng overfitting.
### Tree-based feature selection
- Some classification or regression models such as tree-based models can be used to compute features importance
- This is can be used to discard irrelevant features.
## Ensemble(hoa tau) Methods
- Instead of using a single model, why not using several models at once.
- Diversity(da dang) vs accuracy(chinh xac):
- - An ensemble(quan the) of classifiers must be more accurate than any of its individual members
- - The individual classifiers composing an ensemble must be: better than random, diverse( phai co su khac biet ve du doan)
- ![alt text](image-10.png)
- There is no algorithm that is always the most accurate in all situations.
- Generate(tao ra) a group of base-learners which when combined has higher accuracy. 
- How do we generate a diverse set of base- learners (models) that complement(bo sung) each other?
- - using Bagging and Boosting
- - - â€¢ Generate new datasets by sampling from the original dataset 
uniformly at random (with replacement), and with different 
subsets of features â€¦
- How do we combine(ket hop) the outputs of base learner for maximum accuracy?
Voting, Weighted combination of the outputs â€¦
## Ensemble Methods: Random Forest
- Random forest is an ensemble(cung nhau) of decision trees
- ![alt text](image-11.png)
- Entropy = -$\sum_{i=1}^{c}p_{i} \log_{c} p_{i}$
- The homogeneity is measured using (e.g.) entropy
- - If a subset(tap hop con) is completely homogeneous(dong nhat) the entropy is zero.
- - If a subset is equally divided(chia) (has same proportion of different labels) then it has entropy of one
- Simplified Algorithm
- - Choose T : the number of trees in the ensemble.
- - Choose ğ‘š' < m: ğ‘š is the number of total features, ğ‘šâ€² is 
the number of features used to calculate the best split at each 
node (typically 20%).
- - Choose a training set
- - For each node, randomly choose m' features and calculate the best split
- Use majority voting among all the trees to predict
## VD1: Giáº£ sá»­ báº¡n cÃ³ má»™t táº­p dá»¯ liá»‡u vá»›i 5 há»c sinh, má»—i há»c sinh cÃ³ 4 Ä‘áº·c trÆ°ng:

| Há»c sinh  | Thá»i gian há»c (giá») | Sá»‘ láº§n Ä‘áº¿n lá»›p | Sá»‘ láº§n lÃ m bÃ i táº­p | Sá»‘ buá»•i há»c thÃªm | ÄÃ£ vÆ°á»£t qua ká»³ thi? |
|-----------|---------------------|----------------|--------------------|------------------|---------------------|
| A         | 10                  | 20             | 5                  | 3                | CÃ³                  |
| B         | 8                   | 18             | 3                  | 2                | KhÃ´ng               |
| C         | 12                  | 22             | 4                  | 4                | CÃ³                  |
| D         | 6                   | 15             | 2                  | 1                | KhÃ´ng               |
| E         | 9                   | 19             | 4                  | 3                | CÃ³                  |

## Thuáº­t toÃ¡n Random Forest:

- Chá»n sá»‘ lÆ°á»£ng cÃ¢y trong rá»«ng **T**.
- Chá»n sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n Ä‘iá»ƒm phÃ¢n chia tá»‘t nháº¥t táº¡i má»—i nÃºt, gá»i lÃ  **m'**, vá»›i Ä‘iá»u kiá»‡n **m'** nhá» hÆ¡n tá»•ng sá»‘ Ä‘áº·c trÆ°ng **m**.
- Láº¥y má»™t táº­p huáº¥n luyá»‡n ngáº«u nhiÃªn cho má»—i cÃ¢y.
- Vá»›i má»—i nÃºt, chá»n ngáº«u nhiÃªn **m'** Ä‘áº·c trÆ°ng vÃ  tÃ­nh toÃ¡n Ä‘iá»ƒm phÃ¢n chia tá»‘t nháº¥t.
- Sá»­ dá»¥ng **nguyÃªn táº¯c Ä‘a sá»‘ phiáº¿u** trong táº¥t cáº£ cÃ¡c cÃ¢y Ä‘á»ƒ dá»± Ä‘oÃ¡n.

## CÃ¢y quyáº¿t Ä‘á»‹nh Ä‘Æ¡n giáº£n trong rá»«ng:
                       Thá»i gian há»c > 9?
                      /                    \
                   CÃ³                      KhÃ´ng
                  /                          \
        Sá»‘ láº§n Ä‘áº¿n lá»›p > 18?             Sá»‘ buá»•i há»c thÃªm > 1?
        /               \                 /                \
      CÃ³                 KhÃ´ng         CÃ³                  KhÃ´ng
    ÄÃ£ vÆ°á»£t qua       KhÃ´ng vÆ°á»£t qua   ÄÃ£ vÆ°á»£t qua        KhÃ´ng vÆ°á»£t qua

## Káº¿t quáº£ dá»± Ä‘oÃ¡n tá»« 100 cÃ¢y trong Random Forest:

| Káº¿t quáº£ dá»± Ä‘oÃ¡n | Sá»‘ cÃ¢y dá»± Ä‘oÃ¡n |
|-----------------|----------------|
| CÃ³              | 60             |
| KhÃ´ng           | 40             |

Káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  "VÆ°á»£t qua ká»³ thi" dá»±a trÃªn Ä‘a sá»‘ phiáº¿u.
## Giáº£ sá»­:
- **T** = 5 (sá»‘ lÆ°á»£ng cÃ¢y trong rá»«ng lÃ  5)
- **ğ‘š** = 10 (tá»•ng sá»‘ Ä‘áº·c trÆ°ng)
- **ğ‘š'** = 3 (sá»‘ Ä‘áº·c trÆ°ng Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn táº¡i má»—i nÃºt)

## VÃ­ dá»¥ 2 vá» cÃ¡c cÃ¢y khÃ¡c nhau trong rá»«ng:

#### CÃ¢y 1:
- Táº­p Ä‘áº·c trÆ°ng ngáº«u nhiÃªn: **Tuá»•i**, **Thu nháº­p**, **Giá»›i tÃ­nh**.
- CÃ¢y nÃ y sáº½ phÃ¢n chia dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng nÃ y vÃ  Ä‘Æ°a ra dá»± Ä‘oÃ¡n liá»‡u khÃ¡ch hÃ ng cÃ³ mua sáº£n pháº©m hay khÃ´ng.

#### CÃ¢y 2:
- Táº­p Ä‘áº·c trÆ°ng ngáº«u nhiÃªn: **Lá»‹ch sá»­ mua hÃ ng**, **Khu vá»±c**, **Sá»‘ lÆ°á»£ng sáº£n pháº©m Ä‘Ã£ mua**.
- CÃ¢y 2 cÃ³ cáº¥u trÃºc khÃ¡c vá»›i cÃ¢y 1 do sá»­ dá»¥ng cÃ¡c Ä‘áº·c trÆ°ng vÃ  táº­p dá»¯ liá»‡u khÃ¡c nhau.

#### CÃ¢y 3:
- Táº­p Ä‘áº·c trÆ°ng ngáº«u nhiÃªn: **Tuá»•i**, **Giá»›i tÃ­nh**, **Thu nháº­p**.
- Dá»± Ä‘oÃ¡n cÃ³ thá»ƒ khÃ¡c hoáº·c giá»‘ng vá»›i cÃ¢y 1 vÃ  2 tÃ¹y vÃ o cÃ¡ch phÃ¢n chia dá»¯ liá»‡u.

#### CÃ¢y 4:
- Táº­p Ä‘áº·c trÆ°ng ngáº«u nhiÃªn: **Sá»‘ láº§n tÆ°Æ¡ng tÃ¡c vá»›i sáº£n pháº©m**, **Thu nháº­p**, **Giá»›i tÃ­nh**.

#### CÃ¢y 5:
- Táº­p Ä‘áº·c trÆ°ng ngáº«u nhiÃªn: **Tuá»•i**, **Lá»‹ch sá»­ mua hÃ ng**, **Khu vá»±c**.

### Káº¿t quáº£ cuá»‘i cÃ¹ng:
- Sau khi cÃ¡c cÃ¢y Ä‘Ã£ hoÃ n thÃ nh dá»± Ä‘oÃ¡n, káº¿t quáº£ sáº½ Ä‘Æ°á»£c dá»±a trÃªn **bá» phiáº¿u Ä‘a sá»‘** (majority voting). Náº¿u 3 trong sá»‘ 5 cÃ¢y dá»± Ä‘oÃ¡n ráº±ng khÃ¡ch hÃ ng sáº½ mua sáº£n pháº©m, thÃ¬ dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng sáº½ lÃ  **cÃ³**.


### Random Forest and Features Importance
- Random Forest can compute the importance of each feature. 
So, it can be used for feature selection.
- This is done using the-  mean decrease impurity(tap chat)
## Regularization
### Regularization - Motivation
- ![alt text](image-12.png)
- Small values for parameters $ğœƒ_0$,$ğœƒ_1$, â€¦ , $ğœƒ_ğ‘$
- - Implies a simpler hypothesis(gia thuyet don gian hon)
- -  Less prone to overfitting (giam overfitting)
- So we just modify our cost function as follows
- ![alt text](image-14.png)
- What happens if ğ€ğ€ is set to zero ?
- - This becomes our original cost function. **Overfitting** can happen.
- What happens if ğ€ğ€ is set to an extremely large value?
- - The algorithm might result in **underfitting**.
- ![alt text](image-15.png)
### Regularized Linear Regression
- ![alt text](image-16.png)
- By the way, how can you write ğ¸(ğœƒğœƒ) in a more compact way,using vectors/matrices
- ![alt text](image-17.png)
#### Gradient Descent
- ![alt text](image-18.png)
#### Normal equation
- ![alt text](image-19.png)
### Regularized Logistic Regression (for classification)
- ![alt text](image-20.png)
- ![alt text](image-21.png)