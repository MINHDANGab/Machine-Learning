# Linear Regression
## Giá»›i thiá»‡u
- Má»™t cÄƒn nhÃ  rá»™ng x1 m2, cÃ³ x2 phÃ²ng ngá»§ vÃ  cÃ¡ch trung tÃ¢m thÃ nh phá»‘ x3 km cÃ³ giÃ¡ lÃ  bao nhiÃªu. Giáº£ sá»­ chÃºng ta Ä‘Ã£ cÃ³ sá»‘ liá»‡u thá»‘ng kÃª tá»« 1000 cÄƒn nhÃ  trong thÃ nh phá»‘ Ä‘Ã³, liá»‡u ráº±ng khi cÃ³ má»™t cÄƒn nhÃ  má»›i vá»›i cÃ¡c thÃ´ng sá»‘ vá» diá»‡n tÃ­ch, sá»‘ phÃ²ng ngá»§ vÃ  khoáº£ng cÃ¡ch tá»›i trung tÃ¢m, chÃºng ta cÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c giÃ¡ cá»§a cÄƒn nhÃ  Ä‘Ã³ khÃ´ng? Náº¿u cÃ³ thÃ¬ hÃ m dá»± Ä‘oÃ¡n y=f(x) sáº½ cÃ³ dáº¡ng nhÆ° tháº¿ nÃ o. á» Ä‘Ã¢y x=[x1,x2,x3] lÃ  má»™t vector hÃ ng chá»©a thÃ´ng tin input, y lÃ  má»™t sá»‘ vÃ´ hÆ°á»›ng (scalar) biá»ƒu diá»…n output
- ![alt text](image.png)
- Má»™t hÃ m sá»‘ Ä‘Æ¡n giáº£n nháº¥t cÃ³ thá»ƒ mÃ´ táº£ má»‘i quan há»‡ giá»¯a giÃ¡ nhÃ  vÃ  3 Ä‘áº¡i lÆ°á»£ng Ä‘áº§u vÃ o lÃ :


- $h(x) = \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_0$ ( 1 )
- $\theta_1$ , $\theta_2$ , $\theta_3$ , $\theta_0$  lÃ  cÃ¡c háº±ng sá»‘, w0 cÃ²n Ä‘Æ°á»£c gá»i lÃ  bias. BÃ i toÃ¡n Ä‘i tÃ¬m cÃ¡c há»‡ sá»‘ tá»‘i Æ°u {$\theta_1$ ,$\theta_2$ ,$\theta_3$ ,$\theta_0$ }
- **ChÃº Ã½**: y
  lÃ  giÃ¡ trá»‹ thá»±c cá»§a outcome (dá»±a trÃªn sá»‘ liá»‡u thá»‘ng kÃª chÃºng ta cÃ³ trong táº­p training data), trong khi $\hat{y}$ lÃ  giÃ¡ trá»‹ mÃ  mÃ´ hÃ¬nh Linear Regression dá»± Ä‘oÃ¡n Ä‘Æ°á»£c
## PhÃ¢n tÃ­ch toÃ¡n há»c
### Dáº¡ng cá»§a Linear Regression
- Trong phÆ°Æ¡ng trÃ¬nh (1) phÃ­a trÃªn, náº¿u chÃºng ta Ä‘áº·t $\mathbf{\theta} = [\theta_0, \theta_1, \theta_2, \theta_3]^T $= lÃ  vector (cá»™t) há»‡ sá»‘ cáº§n pháº£i tá»‘i Æ°u
- $\mathbf{\bar{x}} = [1, x_1, x_2, x_3]$ lÃ  vecsto hÃ ng
- Khi Ä‘Ã³ (1) trá»Ÿ thÃ nh: $ h \approx \mathbf{\bar{x}}\mathbf{\theta} = \hat{h}$
### Error Function
- $E(\theta_0, \theta_1, ...) = \frac{1}{2n} \sum_{i=1}^n [h_\theta(x^{(i)}) - h^{(i)}]^2$
- Äá»ƒ tÃ¬m cÃ¡c tham sá»‘ giáº£m thiá»ƒu **Error Function**, chÃºng ta cÃ³ thá»ƒ sá»­ dá»¥ng thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a cÃ³ tÃªn lÃ : **Gradient Descent**
- **Gradient Descent** lÃ  má»™t thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a chung, khÃ´ng chá»‰ dÃ nh riÃªng cho Error Function.
## Gradient Descent 
### Basic idea
1. Start with some values for the parameters $\theta_0$, $\theta_1$
2. Keep updating $\theta_0$, $\theta_1$ to reduce ğ¸($\theta_0$, $\theta_1$) until we hopefully end up at a minimum
- ![alt text](image-1.png)
- Depending on initial(ban Ä‘áº§u) parameter values, we might end-up at a different (local) minimum
- ![alt text](image-2.png)
- At each update, how do we decide if we should increase or decrease each of the parameters ?
### Algorithm
- Pick some initial value for ğœƒ1
- We want to update $\theta_1$ : Î¸â‚ â† Î¸â‚ - Î± â‹… âˆ‚E/âˆ‚Î¸â‚
- **Derivative**: Looks at the slope(Ä‘á»™ dá»‘c) of the (red) line which is tangent (tiáº¿p tuyáº¿n) to the function at that point.
- ![alt text](image-4.png)
- **Positive slope**: In this case, since the derivative is positive and ğ›¼ â‰¥ 0, then ğœƒ1 will decrease, and getâ€™s closer(gáº§n tá»›i) to the optimal (tá»‘i Æ°u) value.
- ![alt text](image-5.png)
- **Negative slope**: In this case, since the derivative is negative and ğ›¼ â‰¥ 0, then ğœƒ1 will increase, and getâ€™s closer to the optimal value.
- ![alt text](image-6.png)
- **Reasonably small value of Î±**
- ![alt text](image-7.png)
- Notice that as we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease ğ›¼ over time
- **Very large value of Î±**
- ![alt text](image-8.png)
- If ğ›¼ is too large, it may fail to 
converge (há»™i tá»¥), or may even diverge (phÃ¢n ká»³).
### Local minimum
- ![alt text](image-9.png)
## Using Gradient Descent for Linear Regression (with one feature)
- ![alt text](image-10.png)
- ![alt text](image-11.png)
- ![alt text](image-12.png)
- **The batch gradient descent** uses all the
training examples, to update the model
parameters.
- **The online (also called stochastic)gradient** descent updates the model
parameters based on(dá»±a trÃªn) one training example at a time(táº¡i má»™t thá»i Ä‘iá»ƒm)
- **eg(1)** : you donâ€™t have all the training dataset beforehand (trÆ°á»›c). Your training examples arrive one by one (tá»«ng cÃ¡i má»™t) over time, as a stream (luá»“ng).
- **e.g(2)**: your training dataset is very big (computationally expensive to use batch
GD, or the dataset doesnâ€™t fit (vá»«a) in memory).
## Multivariate Linear Regression (with multiple features)
- ![alt text](image-13.png)
- $h_\theta(x)= \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_0$ 
- Define $x_0$=1
- ![alt text](image-14.png)
- $h_\theta(x) = \theta^T x$
- ![alt text](image-16.png)
- ![alt text](image-17.png)
## Convergence and Select Î±
- For a sufficiently small(Ä‘á»§ nhá») ğ›¼, the ğ¸(ğœƒ) (on the training set) should decrease at every iteration.
- One can consider convergence (thus stop)
if ğ¸(ğœƒ) decreases by less than ğœ–(small number e.g. 0.0001) in one iteration.
## Linear Regression with the Normal Equation
- Method to solve for ğœƒğœƒ analytically
- The derivative at the optimal point equals to 0. So, set the derivative to 0 
$\frac{\partial}{\partial \theta_j} E(\theta) = ... = 0$, and solve for $\theta_0$, â€¦ , 
- The solution will be: $\theta = (X^TX)^{-1}X^Ty$
## Gradient Descent vs. Normal Equation (for linear regression)
- ![alt text](image-18.png)
# Non-linear regression
- The output $h_\theta$ is a non-linear function
- Polynomial (Äa thá»©c) function of degree(báº­c) > 1
- ![alt text](image-19.png)
- ![alt text](image-20.png)
## Generalized(tá»•ng quÃ¡t) Linear Model for Non-linear Regression
- This can be seen as just creating and adding new features based on(dá»±a vÃ o) the two original features ğ‘¥1, ğ‘¥2
- We can still find the parameters ğœƒ of the non-linear model (in ğ‘¥) using a linear model based on ğ‘§.
- So, we can use the methods we studied previously(trÆ°á»›c Ä‘Ã¢y) in the linear regression lecture(bÃ i giáº£ng).
- It can be any kind of new features
- Requires a good guess of relevant features for your problem â€¦
- VÃ­ dá»¥: GiÃ¡ nhÃ  cÃ³ thá»ƒ khÃ´ng chá»‰ tÄƒng tuyáº¿n tÃ­nh vá»›i diá»‡n tÃ­ch mÃ  cÃ²n cÃ³ thá»ƒ tÄƒng nhanh hÆ¡n hoáº·c cháº­m hÆ¡n tÃ¹y thuá»™c vÃ o diá»‡n tÃ­ch. Äá»ƒ mÃ´ hÃ¬nh hÃ³a Ä‘iá»u nÃ y, chÃºng ta cÃ³ thá»ƒ thÃªm cÃ¡c Ä‘áº·c trÆ°ng má»›i:
## K-Nearest-Neighbors (KNN) for Non-linear Regression
- KNN is a non-parametric model
-  This does not mean parameter-free (e.g. K is a hyper-parameters).
-  **Parametric**: we select a hypothesis(giáº£ thuyáº¿t) space and adjust a fixed set of parameter using the training data.
-  **Non-parametric**: the model is not characterized(Ä‘áº·c trá»©ng) by a fixed set of parameter
- KNN, we just save/memorize the training dataset (there is no training as such).
- To make a prediction on a new data-point ğ‘¥, we look at the ğ‘˜ most similar(tÆ°Æ¡ng tá»±) (i.e. closest/nearest) data-points from the training dataset. We can take:
- the **average output**(trung bÃ¬nh khÃ´ng trá»ng sá»‘) from these ğ‘˜ examples.
- **weighted average output**(trung bÃ¬nh cÃ³ trá»ng sá»‘) from these ğ‘˜ examples.
- ![alt text](image-22.png)
- ![alt text](image-23.png)
vÃ­ dá»¥:
- Giáº£ sá»­ báº¡n cÃ³ táº­p dá»¯ liá»‡u huáº¥n luyá»‡n vá»›i n=5 vÃ­ dá»¥ vÃ  báº¡n sá»­ dá»¥ng ğ‘˜=3:
## VÃ­ dá»¥ chi tiáº¿t vá» thuáº­t toÃ¡n KNN

**Táº­p dá»¯ liá»‡u huáº¥n luyá»‡n:**
* Äiá»ƒm 1: (x1, y1) = (1, 3)
* Äiá»ƒm 2: (x2, y2) = (2, 5)
* Äiá»ƒm 3: (x3, y3) = (3, 7)
* **Äiá»ƒm cáº§n dá»± Ä‘oÃ¡n:** x_má»›i = 2.5

**Chá»n k = 2**

### 1. Trung bÃ¬nh khÃ´ng trá»ng sá»‘
* **CÃ¡c Ä‘iá»ƒm gáº§n nháº¥t:** Äiá»ƒm 2 vÃ  Äiá»ƒm 3
* **TÃ­nh toÃ¡n:**
- Å· = (y2 + y3) / 2 = (5 + 7) / 2 = 6
- ![alt text](image-24.png)
## Kernel Regression
-  Very similar to the weighted KNN method
- A common kernel regression model is the Nadaraya-Watson estimator, with a
**Gaussian kernel function**.
- ![alt text](image-25.png)
- ![alt text](image-26.png)
- Data-points closer to ğ‘¥ have a larger weight(trong so lon hon), i.e. influences(anh huong) the prediction more.
- Larger values of ğœ implies that more data-points will influence the prediction.(ğœ lon nhieu diem du lieu anh huong den du doan)
- Too small ğœ may lead to overfitting(qua khop). Too large ğœ may lead to underfitting(chua khop du)
- MÃ´ hÃ¬nh quÃ¡ khá»›p: KhÃ´ng cÃ³ Ä‘iá»ƒm dá»¯ liá»‡u nÃ o Ä‘á»§ gáº§n Ä‘á»ƒ áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n dá»± Ä‘oÃ¡n, dáº«n Ä‘áº¿n má»™t mÃ´ hÃ¬nh khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c.
- MÃ´ hÃ¬nh Ä‘áº¡t chuáº©n: GiÃ¡ trá»‹ ğœ vá»«a pháº£i giÃºp cÃ¢n báº±ng giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gáº§n vÃ  xa, táº¡o ra má»™t dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c vÃ  tá»•ng quÃ¡t hÆ¡n.
- MÃ´ hÃ¬nh chÆ°a khá»›p Ä‘á»§: GiÃ¡ trá»‹ ğœ quÃ¡ lá»›n khiáº¿n mÃ´ hÃ¬nh khÃ´ng Ä‘á»§ nháº¡y vá»›i sá»± thay Ä‘á»•i cá»§a dá»¯ liá»‡u vÃ  dá»± Ä‘oÃ¡n chá»‰ gáº§n vá»›i trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c giÃ¡ trá»‹.
## Features Normalization / Scaling
- Feature normalization is a preprocessing (tien xu li)step used to normalize(chuan hoa) the range(pham vi) of the features(tinh nang)
- It is important when the features have very different scales
- If one of the features has a broad range(khoang rong) of values, the distance will be governed(chi phoi) by this particular feature. Therefore, the range of all features should be 
normalized so that each feature contributes(gop phan) approximately(xap xo) proportionately(tuong ung) to the final distance. 
### Min-max Features scaling
- ![alt text](image-27.png)
- These values will be âˆˆ [0, 1]
- ![alt text](image-28.png)
- 

